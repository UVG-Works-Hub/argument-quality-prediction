{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Final: Exportación de Modelos y Datos para la Aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Para visualizar los resultados de SHAP en el notebook (opcional)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to DIR\n",
    "SAVE2DIR = '../saved/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar y Preprocesar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos preprocesados\n",
    "train_df = pd.read_csv('../data/train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>317</td>\n",
       "      <td>hi im isaac im going writing face mars natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>210</td>\n",
       "      <td>perspective think face natural landform dont t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>105</td>\n",
       "      <td>think face natural landform life mars descover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>362</td>\n",
       "      <td>life mars would know reason think natural land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>101</td>\n",
       "      <td>people thought face formed alieans thought lif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  text_length  \\\n",
       "0                Adequate          317   \n",
       "1                Adequate          210   \n",
       "2                Adequate          105   \n",
       "3                Adequate          362   \n",
       "4                Adequate          101   \n",
       "\n",
       "                                                text  \n",
       "0  hi im isaac im going writing face mars natural...  \n",
       "1  perspective think face natural landform dont t...  \n",
       "2  think face natural landform life mars descover...  \n",
       "3  life mars would know reason think natural land...  \n",
       "4  people thought face formed alieans thought lif...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar y manejar valores nulos\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/encoders/label_encoder.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificar las etiquetas de clase\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(train_df['discourse_effectiveness'])\n",
    "joblib.dump(le, SAVE2DIR + 'encoders/label_encoder.pkl')  # Guardar el LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/encoders/tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorización del texto\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_text = tfidf.fit_transform(train_df['text'])\n",
    "joblib.dump(tfidf, SAVE2DIR + 'encoders/tfidf_vectorizer.pkl')  # Guardar el TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/encoders/standard_scaler.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escalar la característica de longitud del texto\n",
    "scaler = StandardScaler()\n",
    "X_length = scaler.fit_transform(train_df[['text_length']])\n",
    "joblib.dump(scaler, SAVE2DIR + 'encoders/standard_scaler.pkl')  # Guardar el StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/encoders/onehot_encoder.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificar 'discourse_type' con OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "X_discourse = ohe.fit_transform(train_df[['discourse_type']])\n",
    "joblib.dump(ohe, SAVE2DIR + 'encoders/onehot_encoder.pkl')  # Guardar el OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Counterclaim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36760</th>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36761</th>\n",
       "      <td>Claim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>Position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>Evidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36764</th>\n",
       "      <td>Concluding Statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36757 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             discourse_type\n",
       "0                      Lead\n",
       "1                  Position\n",
       "2                     Claim\n",
       "3                  Evidence\n",
       "4              Counterclaim\n",
       "...                     ...\n",
       "36760                 Claim\n",
       "36761                 Claim\n",
       "36762              Position\n",
       "36763              Evidence\n",
       "36764  Concluding Statement\n",
       "\n",
       "[36757 rows x 1 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['discourse_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar características\n",
    "import scipy.sparse as sp\n",
    "X = sp.hstack([X_text, X_length, X_discourse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener nombres de las características para futuras referencias\n",
    "tfidf_features = tfidf.get_feature_names_out()\n",
    "length_feature = ['text_length']\n",
    "discourse_features = ohe.get_feature_names_out(['discourse_type'])\n",
    "feature_names = list(tfidf_features) + length_feature + list(discourse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['discourse_type_Concluding Statement',\n",
       "       'discourse_type_Counterclaim', 'discourse_type_Evidence',\n",
       "       'discourse_type_Lead', 'discourse_type_Position',\n",
       "       'discourse_type_Rebuttal'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.get_feature_names_out(['discourse_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenar Modelos con Mejores Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/models/logistic_regression.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar Logistic Regression\n",
    "log_reg = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    max_iter=1000,\n",
    "    C=1,\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "log_reg.fit(X_train, y_train)\n",
    "joblib.dump(log_reg, SAVE2DIR + 'models/logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved/models/xgboost.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "joblib.dump(xgb, SAVE2DIR + 'models/xgboost.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenar Modelos de Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros comunes\n",
    "num_classes = len(le.classes_)\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Modelo Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - accuracy: 0.6079 - loss: 0.8853 - val_accuracy: 0.6692 - val_loss: 0.7487\n",
      "Epoch 2/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.6969 - loss: 0.6878 - val_accuracy: 0.6600 - val_loss: 0.7554\n",
      "Epoch 3/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.7400 - loss: 0.5986 - val_accuracy: 0.6443 - val_loss: 0.7850\n",
      "Epoch 4/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.8005 - loss: 0.4911 - val_accuracy: 0.6362 - val_loss: 0.8501\n",
      "Epoch 5/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.8543 - loss: 0.3828 - val_accuracy: 0.6386 - val_loss: 0.9625\n",
      "Epoch 6/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9009 - loss: 0.2772 - val_accuracy: 0.6268 - val_loss: 1.1478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "def create_keras_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Crear y compilar el modelo Keras\n",
    "keras_nn = create_keras_model(input_dim, num_classes)\n",
    "keras_nn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Definir Early Stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo Keras\n",
    "history = keras_nn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo Keras\n",
    "keras_nn.save(SAVE2DIR + 'models/keras_nn_model.h5')\n",
    "\n",
    "# Guardar el historial de entrenamiento\n",
    "with open(SAVE2DIR + 'metrics/keras_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Modelo PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Inicializar el modelo PyTorch\n",
    "pytorch_nn = NeuralNetPyTorch(input_dim, num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pytorch_nn.to(device)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pytorch_nn.parameters(), lr=0.001)\n",
    "\n",
    "# Preparar datos para PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.toarray().astype('float32')\n",
    "        self.y = y.astype('int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset_pytorch = TextDataset(X_train, y_train)\n",
    "val_dataset_pytorch = TextDataset(X_val, y_val)\n",
    "\n",
    "train_loader_pytorch = DataLoader(train_dataset_pytorch, batch_size=128, shuffle=True)\n",
    "val_loader_pytorch = DataLoader(val_dataset_pytorch, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetPyTorch(\n",
       "  (fc1): Linear(in_features=5007, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir el dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pytorch_nn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.8236 - Val Loss: 0.7551 - Val Acc: 0.6616\n",
      "Epoch 2/50 - Train Loss: 0.6970 - Val Loss: 0.7621 - Val Acc: 0.6685\n",
      "Epoch 3/50 - Train Loss: 0.6317 - Val Loss: 0.7786 - Val Acc: 0.6511\n",
      "Epoch 4/50 - Train Loss: 0.5615 - Val Loss: 0.8366 - Val Acc: 0.6513\n",
      "Epoch 5/50 - Train Loss: 0.4772 - Val Loss: 0.8888 - Val Acc: 0.6322\n",
      "Epoch 6/50 - Train Loss: 0.3836 - Val Loss: 0.9940 - Val Acc: 0.6235\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo PyTorch\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "train_losses_pytorch = []\n",
    "val_losses_pytorch = []\n",
    "val_accuracies_pytorch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pytorch_nn.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader_pytorch:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pytorch_nn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader_pytorch.dataset)\n",
    "    train_losses_pytorch.append(epoch_loss)\n",
    "\n",
    "    # Validación\n",
    "    pytorch_nn.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_pytorch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = pytorch_nn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader_pytorch.dataset)\n",
    "    val_losses_pytorch.append(val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies_pytorch.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        # Guardar el mejor modelo\n",
    "        torch.save(pytorch_nn.state_dict(), SAVE2DIR + 'models/best_pytorch_nn_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetPyTorch(\n",
       "  (fc1): Linear(in_features=5007, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el mejor modelo PyTorch\n",
    "pytorch_nn.load_state_dict(torch.load(SAVE2DIR + 'models/best_pytorch_nn_model.pth'))\n",
    "pytorch_nn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las métricas de PyTorch\n",
    "metrics_pytorch = {\n",
    "    'train_losses': train_losses_pytorch,\n",
    "    'val_losses': val_losses_pytorch,\n",
    "    'val_accuracies': val_accuracies_pytorch\n",
    "}\n",
    "\n",
    "with open(SAVE2DIR + 'metrics/pytorch_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics_pytorch, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluación de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "\n",
    "models_performance = {}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(name, model, X_val, y_val, is_nn=False, device='cpu'):\n",
    "    if is_nn:\n",
    "        if name == 'PyTorch NN':\n",
    "            # Check if X_val is a sparse matrix\n",
    "            if scipy.sparse.issparse(X_val):\n",
    "                # Convert sparse matrix to dense\n",
    "                X_val_dense = X_val.toarray()\n",
    "            else:\n",
    "                X_val_dense = X_val\n",
    "            # Convert to PyTorch tensor\n",
    "            X_val_tensor = torch.tensor(X_val_dense, dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val_tensor)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_pred = preds.cpu().numpy()\n",
    "        else:  # Keras NN\n",
    "            # Keras can handle sparse matrices if they're converted to dense\n",
    "            if scipy.sparse.issparse(X_val):\n",
    "                X_val_dense = X_val.toarray()\n",
    "            else:\n",
    "                X_val_dense = X_val\n",
    "            y_pred = np.argmax(model.predict(X_val_dense), axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    report = classification_report(y_val, y_pred, target_names=le.classes_, output_dict=True)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    performance = {\n",
    "        'accuracy': acc,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    models_performance[name] = performance\n",
    "\n",
    "    # Add feature importances if applicable\n",
    "    if name in ['Random Forest', 'XGBoost']:\n",
    "        feature_importances = [{'feature': feature, 'importance': importance}\n",
    "                               for feature, importance in zip(feature_names, model.feature_importances_)]\n",
    "        # Save to a csv file\n",
    "        pd.DataFrame(feature_importances).to_csv(SAVE2DIR + f'metrics/{name}_feature_importances.csv', index=False)\n",
    "        # performance['feature_importances'] = feature_importances\n",
    "\n",
    "    elif name == 'Logistic Regression':\n",
    "        # Logistic Regression feature importances are based on coefficients\n",
    "        feature_importances = [{'feature': feature, 'importance': coef}\n",
    "                               for feature, coef in zip(feature_names, model.coef_[0])]\n",
    "        # Save to a csv file\n",
    "        pd.DataFrame(feature_importances).to_csv(SAVE2DIR + 'metrics/logistic_regression_feature_importances.csv', index=False)\n",
    "\n",
    "# Example evaluations\n",
    "# Evaluate Logistic Regression\n",
    "evaluate_model('Logistic Regression', log_reg, X_val, y_val)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "evaluate_model('XGBoost', xgb, X_val, y_val)\n",
    "\n",
    "# Evaluate Keras NN\n",
    "evaluate_model('Keras NN', keras_nn, X_val, y_val, is_nn=True)\n",
    "\n",
    "# Evaluate PyTorch NN\n",
    "evaluate_model('PyTorch NN', pytorch_nn, X_val, y_val, is_nn=True, device=device)\n",
    "\n",
    "# Save model performance metrics\n",
    "with open(SAVE2DIR + 'metrics/models_performance.pkl', 'wb') as f:\n",
    "    pickle.dump(models_performance, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extraer y Guardar Importancias de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Importancias de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar importancias de XGBoost\n",
    "if 'XGBoost' in models_performance:\n",
    "    xgb_importances = models_performance['XGBoost']['feature_importances']\n",
    "    xgb_importances_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': xgb_importances\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    xgb_importances_df.to_csv(SAVE2DIR + 'metrics/xgboost_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Importancia de las características SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Importancia de las características para Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando SHAP para el modelo Keras NN...\n",
      "Length of feature_names: 5007\n",
      "Shape of shap_importances_keras: (5007, 3)\n",
      "Mismatch detected between feature names and SHAP importances for Keras NN.\n",
      "Length of feature_names_aligned: 5007\n",
      "Shape of shap_importances_keras after alignment: (5007, 3)\n",
      "Guardando importancias de características SHAP para Keras NN...\n",
      "[0. 0. 0.]\n",
      "Class: Adequate\n",
      "Class index: 0\n",
      "Class: Effective\n",
      "Class index: 1\n",
      "Class: Ineffective\n",
      "Class index: 2\n",
      "Importancias de características SHAP para Keras NN guardadas en 'metrics/keras_nn_shap_importances.csv'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculando SHAP para el modelo Keras NN...\")\n",
    "\n",
    "# Convertir X_val a denso si es necesario\n",
    "if scipy.sparse.issparse(X_val):\n",
    "    X_val_dense = X_val.toarray()\n",
    "else:\n",
    "    X_val_dense = X_val\n",
    "\n",
    "# Seleccionar una muestra para acelerar el cálculo de SHAP (opcional)\n",
    "shap_sample_size = 10  # Puedes ajustar este número según tus recursos\n",
    "X_shap = X_val_dense[:shap_sample_size]\n",
    "\n",
    "# Crear el explainer usando DeepExplainer\n",
    "explainer_keras = shap.DeepExplainer(keras_nn, X_shap)\n",
    "\n",
    "# Calcular los valores SHAP\n",
    "shap_values_keras = explainer_keras.shap_values(X_shap)\n",
    "\n",
    "# Agregar los valores SHAP para todas las clases\n",
    "# Para clasificación multiclase, SHAP devuelve una lista de arrays, uno por clase\n",
    "if isinstance(shap_values_keras, list):\n",
    "    # Cada elemento de la lista corresponde a una clase\n",
    "    print(f\"GG Number of classes: {len(shap_values_keras)}\")\n",
    "    shap_importances_keras = np.array([\n",
    "        np.abs(shap_val).mean(axis=0) for shap_val in shap_values_keras\n",
    "    ])  # Shape: (num_classes, num_features)\n",
    "else:\n",
    "    shap_importances_keras = np.abs(shap_values_keras).mean(axis=0)  # Shape: (num_features,)\n",
    "\n",
    "# Verificar las longitudes\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "print(f\"Shape of shap_importances_keras: {shap_importances_keras.shape}\")\n",
    "\n",
    "# Alinear las longitudes si es necesario\n",
    "if len(feature_names) != shap_importances_keras.shape[-1]:\n",
    "    print(\"Mismatch detected between feature names and SHAP importances for Keras NN.\")\n",
    "    # Investigar la causa del desajuste\n",
    "    # Por ahora, truncamos ambas listas al tamaño mínimo\n",
    "    min_length = min(len(feature_names), shap_importances_keras.shape[-1])\n",
    "    feature_names_aligned = feature_names\n",
    "    shap_importances_keras = shap_importances_keras[:, :min_length] if isinstance(shap_importances_keras, np.ndarray) else shap_importances_keras[:min_length]\n",
    "else:\n",
    "    feature_names_aligned = feature_names\n",
    "\n",
    "# Verificar nuevamente las dimensiones después de la alineación\n",
    "print(f\"Length of feature_names_aligned: {len(feature_names_aligned)}\")\n",
    "if isinstance(shap_importances_keras, np.ndarray):\n",
    "    print(f\"Shape of shap_importances_keras after alignment: {shap_importances_keras.shape}\")\n",
    "else:\n",
    "    print(f\"Length of shap_importances_keras after alignment: {len(shap_importances_keras)}\")\n",
    "\n",
    "print(\"Guardando importancias de características SHAP para Keras NN...\")\n",
    "\n",
    "# Crear un DataFrame con las importancias por clase\n",
    "shap_importances_df_keras = pd.DataFrame({\n",
    "    'feature': feature_names_aligned\n",
    "})\n",
    "\n",
    "print(shap_importances_keras[1000])\n",
    "\n",
    "# Añadir una columna por cada clase\n",
    "for class_idx, class_name in enumerate(le.classes_):\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(f\"Class index: {class_idx}\")\n",
    "\n",
    "    # for each element in shap_importances_keras, get the class_idx element\n",
    "    class_importance = []\n",
    "    for i in range(len(shap_importances_keras)):\n",
    "        class_importance.append(shap_importances_keras[i][class_idx])\n",
    "\n",
    "    shap_importances_df_keras[f'shap_importance_class_{class_name}'] = class_importance\n",
    "\n",
    "# Ordenar el DataFrame según la importancia de la primera clase (puedes elegir otra clase o usar un promedio)\n",
    "shap_importances_df_keras = shap_importances_df_keras.sort_values(by=f'shap_importance_class_{le.classes_[0]}', ascending=False)\n",
    "\n",
    "# Guardar las importancias de SHAP para Keras\n",
    "shap_importances_df_keras.to_csv(SAVE2DIR + 'metrics/keras_nn_shap_importances.csv', index=False)\n",
    "\n",
    "print(\"Importancias de características SHAP para Keras NN guardadas en 'metrics/keras_nn_shap_importances.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Importancia de las características para PyTorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando SHAP para el modelo PyTorch NN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291fcde79ead4b7daa45adcb51763b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature_names: 5007\n",
      "Shape of shap_importances_pytorch: (5007, 3)\n",
      "Mismatch detected between feature names and SHAP importances for PyTorch NN.\n",
      "Length of feature_names_aligned_pytorch: 5007\n",
      "Shape of shap_importances_pytorch after alignment: (5007, 3)\n",
      "Guardando importancias de características SHAP para PyTorch NN...\n",
      "Importancias de características SHAP para PyTorch NN guardadas en 'metrics/pytorch_nn_shap_importances.csv'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculando SHAP para el modelo PyTorch NN...\")\n",
    "\n",
    "# Definir una función de predicción para PyTorch\n",
    "def pytorch_predict(x):\n",
    "    pytorch_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        outputs = pytorch_nn(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Crear el explainer usando KernelExplainer\n",
    "explainer_pytorch = shap.KernelExplainer(pytorch_predict, X_shap)\n",
    "\n",
    "# Calcular los valores SHAP\n",
    "shap_values_pytorch = explainer_pytorch.shap_values(X_shap, nsamples=100)\n",
    "\n",
    "# Agregar los valores SHAP para todas las clases\n",
    "if isinstance(shap_values_pytorch, list):\n",
    "    shap_importances_pytorch = np.array([\n",
    "        np.abs(shap_val).mean(axis=0) for shap_val in shap_values_pytorch\n",
    "    ])  # Shape: (num_classes, num_features)\n",
    "else:\n",
    "    shap_importances_pytorch = np.abs(shap_values_pytorch).mean(axis=0)  # Shape: (num_features,)\n",
    "\n",
    "# Verificar las longitudes\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "print(f\"Shape of shap_importances_pytorch: {shap_importances_pytorch.shape}\")\n",
    "\n",
    "# Alinear las longitudes si es necesario\n",
    "if len(feature_names) != shap_importances_pytorch.shape[-1]:\n",
    "    print(\"Mismatch detected between feature names and SHAP importances for PyTorch NN.\")\n",
    "    # Investigar la causa del desajuste\n",
    "    # Por ahora, truncamos ambas listas al tamaño mínimo\n",
    "    min_length = min(len(feature_names), shap_importances_pytorch.shape[-1])\n",
    "    feature_names_aligned_pytorch = feature_names\n",
    "    shap_importances_pytorch = shap_importances_pytorch[:, :min_length] if isinstance(shap_importances_pytorch, np.ndarray) else shap_importances_pytorch[:min_length]\n",
    "else:\n",
    "    feature_names_aligned_pytorch = feature_names\n",
    "\n",
    "# Verificar nuevamente las dimensiones después de la alineación\n",
    "print(f\"Length of feature_names_aligned_pytorch: {len(feature_names_aligned_pytorch)}\")\n",
    "if isinstance(shap_importances_pytorch, np.ndarray):\n",
    "    print(f\"Shape of shap_importances_pytorch after alignment: {shap_importances_pytorch.shape}\")\n",
    "else:\n",
    "    print(f\"Length of shap_importances_pytorch after alignment: {len(shap_importances_pytorch)}\")\n",
    "\n",
    "print(\"Guardando importancias de características SHAP para PyTorch NN...\")\n",
    "\n",
    "# Crear un DataFrame con las importancias por clase\n",
    "shap_importances_df_pytorch = pd.DataFrame({\n",
    "    'feature': feature_names_aligned_pytorch\n",
    "})\n",
    "\n",
    "# Añadir una columna por cada clase\n",
    "for class_idx, class_name in enumerate(le.classes_):\n",
    "    class_importance = []\n",
    "    for i in range(len(shap_importances_pytorch)):\n",
    "        class_importance.append(shap_importances_pytorch[i][class_idx])\n",
    "\n",
    "    shap_importances_df_pytorch[f'shap_importance_class_{class_name}'] = class_importance\n",
    "\n",
    "# Ordenar el DataFrame según la importancia de la primera clase (puedes elegir otra clase o usar un promedio)\n",
    "shap_importances_df_pytorch = shap_importances_df_pytorch.sort_values(by=f'shap_importance_class_{le.classes_[0]}', ascending=False)\n",
    "\n",
    "# Guardar las importancias de SHAP para PyTorch\n",
    "shap_importances_df_pytorch.to_csv(SAVE2DIR + 'metrics/pytorch_nn_shap_importances.csv', index=False)\n",
    "\n",
    "print(\"Importancias de características SHAP para PyTorch NN guardadas en 'metrics/pytorch_nn_shap_importances.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar Métricas Adicionales para Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Todos los modelos, codificadores y métricas han sido guardados correctamente!\n"
     ]
    }
   ],
   "source": [
    "# 9.1. Guardar las matrices de confusión como datos\n",
    "for model_name, metrics in models_performance.items():\n",
    "    cm = metrics['confusion_matrix']\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    cm_df.to_csv(SAVE2DIR + f'metrics/confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.csv')\n",
    "\n",
    "# 9.2. Guardar las precisiones de los modelos\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Model': list(models_performance.keys()),\n",
    "    'Accuracy': [metrics['accuracy'] for metrics in models_performance.values()]\n",
    "}).sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "accuracy_df.to_csv(SAVE2DIR +'metrics/model_accuracies.csv', index=False)\n",
    "\n",
    "# 9.3. Guardar informes de clasificación\n",
    "for model_name, metrics in models_performance.items():\n",
    "    report = metrics['classification_report']\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(SAVE2DIR + f'metrics/classification_report_{model_name.replace(\" \", \"_\").lower()}.csv')\n",
    "\n",
    "# 10. Guardar Información Adicional del Dataset para Gráficas\n",
    "\n",
    "# 10.1. Estadísticas de Longitud de Texto\n",
    "text_length_stats = train_df['text_length'].describe().to_frame().T\n",
    "text_length_stats.to_csv(SAVE2DIR + 'metrics/text_length_stats.csv', index=False)\n",
    "\n",
    "# 10.2. Distribución de Tipos de Discurso\n",
    "discourse_type_counts = train_df['discourse_type'].value_counts().reset_index()\n",
    "discourse_type_counts.columns = ['discourse_type', 'count']\n",
    "discourse_type_counts.to_csv(SAVE2DIR + 'metrics/discourse_type_counts.csv', index=False)\n",
    "\n",
    "# 10.3. Distribución de Efectividad del Discurso\n",
    "effectiveness_counts = train_df['discourse_effectiveness'].value_counts().reset_index()\n",
    "effectiveness_counts.columns = ['discourse_effectiveness', 'count']\n",
    "effectiveness_counts.to_csv(SAVE2DIR + 'metrics/discourse_effectiveness_counts.csv', index=False)\n",
    "\n",
    "# 10.4. Guardar el Dataset Procesado (Opcional)\n",
    "train_df.to_csv(SAVE2DIR + 'metrics/processed_train_dataset.csv', index=False)\n",
    "\n",
    "print(\"¡Todos los modelos, codificadores y métricas han sido guardados correctamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
