{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Classification of Argumentative Elements in Student Writing\n",
    "\n",
    "## Tabla de Contenidos\n",
    "1. [Introduction](#Introducción)\n",
    "2. [1. Selección de Algoritmos](#1-Selección-de-Algoritmos)\n",
    "3. [2. Preprocesamiento e Ingeniería de Características](#2-Preprocesamiento-e-Ingeniería-de-Características)\n",
    "4. [3. Implementación de Modelos](#3-Implementación-de-Modelos)\n",
    "5. [4. Evaluación y Comparación de Modelos](#4-Evaluación-y-Comparación-de-Modelos)\n",
    "6. [5. Visualización y Comunicación de Resultados](#5-Visualización-y-Comunicación-de-Resultados)\n",
    "7. [Conclusiones](#Conclusiones)\n",
    "8. [Referencias](#Referencias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El objetivo de este proyecto se basa en crear modelos de machine para clasificar los elementos argumentativos en los textos escritos por estudiantes como \"efectivos\", \"adecuados\" o \"ineficaces\". Esta clasificación automatizada tiene como objetivo proporcionar una mejor retroalimentación a los estudiantes, para así ellos puedan mejorar sus habilidades de escritura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selección de Algoritmos\n",
    "\n",
    "### Investigación y Selección de Algoritmos\n",
    "\n",
    "Para abordar el problema de clasificación de elementos argumentativos en textos de estudiantes, se seleccionaron los siguientes cinco algoritmos de Machine Learning:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest Classifier**\n",
    "3. **XGBoost**\n",
    "4. **Red Neuronal (Neural Network) con Keras**\n",
    "5. **Red Neuronal (Neural Network) con PyTorch**\n",
    "\n",
    "### Justificación de la Selección\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - **Características**: Modelo lineal simple que es eficiente y fácil de interpretar.\n",
    "   - **Justificación**: Adecuado para problemas de clasificación multiclase y sirve como un buen punto de referencia.\n",
    "   - **Referencia**: [Hosmer, D.W., Lemeshow, S. (2000). Applied Logistic Regression. Wiley.]\n",
    "\n",
    "2. **Random Forest Classifier**:\n",
    "   - **Características**: Ensamble de árboles de decisión que maneja bien las interacciones no lineales y la importancia de características.\n",
    "   - **Justificación**: Eficaz para manejar conjuntos de datos con características complejas y evita el sobreajuste.\n",
    "   - **Referencia**: [Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.]\n",
    "\n",
    "3. **XGBoost**:\n",
    "   - **Características**: Algoritmo de gradiente potenciado que es altamente eficiente y preciso, especialmente en problemas de clasificación y regresión.\n",
    "   - **Justificación**: Excelente para manejar características de alta dimensionalidad y ofrece mecanismos avanzados de regularización para evitar el sobreajuste.\n",
    "   - **Referencia**: [Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.]\n",
    "\n",
    "4. **Red Neuronal (Neural Network) con Keras**:\n",
    "   - **Características**: Modelos capaces de capturar relaciones complejas y no lineales en los datos mediante múltiples capas de neuronas.\n",
    "   - **Justificación**: Adecuada para procesamiento de texto y puede aprovechar representaciones de características profundas para mejorar la clasificación.\n",
    "   - **Referencia**: [Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.]\n",
    "\n",
    "5. **Red Neuronal (Neural Network) con PyTorch**:\n",
    "   - **Características**: Similar a las redes neuronales con Keras, pero con mayor flexibilidad y control en el proceso de entrenamiento.\n",
    "   - **Justificación**: Permite una personalización más detallada del modelo y el proceso de entrenamiento, lo que puede mejorar el rendimiento en tareas específicas.\n",
    "   - **Referencia**: [Paszke, A., et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information Processing Systems.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento e Ingeniería de Características\n",
    "\n",
    "### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>317</td>\n",
       "      <td>hi im isaac im going writing face mars natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>210</td>\n",
       "      <td>perspective think face natural landform dont t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>105</td>\n",
       "      <td>think face natural landform life mars descover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>362</td>\n",
       "      <td>life mars would know reason think natural land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>101</td>\n",
       "      <td>people thought face formed alieans thought lif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  text_length  \\\n",
       "0                Adequate          317   \n",
       "1                Adequate          210   \n",
       "2                Adequate          105   \n",
       "3                Adequate          362   \n",
       "4                Adequate          101   \n",
       "\n",
       "                                                text  \n",
       "0  hi im isaac im going writing face mars natural...  \n",
       "1  perspective think face natural landform dont t...  \n",
       "2  think face natural landform life mars descover...  \n",
       "3  life mars would know reason think natural land...  \n",
       "4  people thought face formed alieans thought lif...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos preprocesados\n",
    "train_df = pd.read_csv('data/train_preprocessed.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Visualizar las primeras filas del conjunto de entrenamiento\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de Datos\n",
    "\n",
    "Aunque ya se ha realizado un limpieza previa, esta es solo una medida de seguridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "train_df.isnull().sum()\n",
    "\n",
    "# Rellenar o eliminar valores nulos si es necesario\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de Características (Feature Engineering)\n",
    "\n",
    "1. **Tokenización y Normalización del Texto**:\n",
    "   - Convertir texto a minúsculas.\n",
    "   - Eliminar puntuación y caracteres especiales.\n",
    "   - Tokenizar palabras.\n",
    "\n",
    "   > Esto fue realizado en la etapa previa (Normalización)\n",
    "\n",
    "2. **Vectorización**:\n",
    "   - Utilizar TF-IDF para transformar el texto en vectores numéricos.\n",
    "\n",
    "3. **Características Adicionales**:\n",
    "   - Longitud del texto (`text_length`).\n",
    "   - Número de palabras clave específicas relacionadas con elementos argumentativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Vectorización del texto\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_text = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "# Escalar la característica de longitud del texto\n",
    "scaler = StandardScaler()\n",
    "X_length = scaler.fit_transform(train_df[['text_length']])\n",
    "\n",
    "# Concatenar características\n",
    "import scipy.sparse as sp\n",
    "X = sp.hstack([X_text, X_length])\n",
    "\n",
    "# Variable objetivo\n",
    "y = train_df['discourse_effectiveness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justificación de las Técnicas Aplicadas\n",
    "\n",
    "- **TF-IDF**: Captura la importancia relativa de las palabras en el contexto del corpus, lo que es esencial para entender la efectividad argumentativa.\n",
    "- **Escalado de Características**: Normaliza la longitud del texto para que no domine otras características durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación de Modelos\n",
    "\n",
    "### División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chama\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inicializar modelos tradicionales\n",
    "log_reg = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)\n",
    "rand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Preparar datos para la Red Neuronal con Keras\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "\n",
    "# Definir el modelo de Red Neuronal con Keras\n",
    "def create_nn_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "num_classes = y.nunique()\n",
    "nn_model_keras = create_nn_model(X_train.shape[1], num_classes)\n",
    "nn_model_keras.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Preparar datos para la Red Neuronal con PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.toarray().astype('float32')\n",
    "        self.y = y.astype('int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset_pytorch = TextDataset(X_train, y_train_enc)\n",
    "val_dataset_pytorch = TextDataset(X_val, y_val_enc)\n",
    "\n",
    "train_loader_pytorch = DataLoader(train_dataset_pytorch, batch_size=128, shuffle=True)\n",
    "val_loader_pytorch = DataLoader(val_dataset_pytorch, batch_size=128, shuffle=False)\n",
    "\n",
    "# Definir el modelo de Red Neuronal con PyTorch\n",
    "class NeuralNetPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "nn_model_pytorch = NeuralNetPyTorch(X_train.shape[1], num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
