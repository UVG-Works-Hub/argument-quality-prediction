{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Classification of Argumentative Elements in Student Writing\n",
    "\n",
    "## Tabla de Contenidos\n",
    "1. [Introduction](#Introducción)\n",
    "2. [1. Selección de Algoritmos](#1-Selección-de-Algoritmos)\n",
    "3. [2. Preprocesamiento e Ingeniería de Características](#2-Preprocesamiento-e-Ingeniería-de-Características)\n",
    "4. [3. Implementación de Modelos](#3-Implementación-de-Modelos)\n",
    "5. [4. Evaluación y Comparación de Modelos](#4-Evaluación-y-Comparación-de-Modelos)\n",
    "6. [5. Visualización y Comunicación de Resultados](#5-Visualización-y-Comunicación-de-Resultados)\n",
    "7. [Conclusiones](#Conclusiones)\n",
    "8. [Referencias](#Referencias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El objetivo de este proyecto se basa en crear modelos de machine para clasificar los elementos argumentativos en los textos escritos por estudiantes como \"efectivos\", \"adecuados\" o \"ineficaces\". Esta clasificación automatizada tiene como objetivo proporcionar una mejor retroalimentación a los estudiantes, para así ellos puedan mejorar sus habilidades de escritura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selección de Algoritmos\n",
    "\n",
    "### Investigación y Selección de Algoritmos\n",
    "\n",
    "Para abordar el problema de clasificación de elementos argumentativos en textos de estudiantes, se seleccionaron los siguientes cinco algoritmos de Machine Learning:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest Classifier**\n",
    "3. **XGBoost**\n",
    "4. **Red Neuronal (Neural Network) con Keras**\n",
    "5. **Red Neuronal (Neural Network) con PyTorch**\n",
    "\n",
    "### Justificación de la Selección\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - **Características**: Modelo lineal simple que es eficiente y fácil de interpretar.\n",
    "   - **Justificación**: Adecuado para problemas de clasificación multiclase y sirve como un buen punto de referencia.\n",
    "   - **Referencia**: [Hosmer, D.W., Lemeshow, S. (2000). Applied Logistic Regression. Wiley.]\n",
    "\n",
    "2. **Random Forest Classifier**:\n",
    "   - **Características**: Ensamble de árboles de decisión que maneja bien las interacciones no lineales y la importancia de características.\n",
    "   - **Justificación**: Eficaz para manejar conjuntos de datos con características complejas y evita el sobreajuste.\n",
    "   - **Referencia**: [Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.]\n",
    "\n",
    "3. **XGBoost**:\n",
    "   - **Características**: Algoritmo de gradiente potenciado que es altamente eficiente y preciso, especialmente en problemas de clasificación y regresión.\n",
    "   - **Justificación**: Excelente para manejar características de alta dimensionalidad y ofrece mecanismos avanzados de regularización para evitar el sobreajuste.\n",
    "   - **Referencia**: [Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.]\n",
    "\n",
    "4. **Red Neuronal (Neural Network) con Keras**:\n",
    "   - **Características**: Modelos capaces de capturar relaciones complejas y no lineales en los datos mediante múltiples capas de neuronas.\n",
    "   - **Justificación**: Adecuada para procesamiento de texto y puede aprovechar representaciones de características profundas para mejorar la clasificación.\n",
    "   - **Referencia**: [Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.]\n",
    "\n",
    "5. **Red Neuronal (Neural Network) con PyTorch**:\n",
    "   - **Características**: Similar a las redes neuronales con Keras, pero con mayor flexibilidad y control en el proceso de entrenamiento.\n",
    "   - **Justificación**: Permite una personalización más detallada del modelo y el proceso de entrenamiento, lo que puede mejorar el rendimiento en tareas específicas.\n",
    "   - **Referencia**: [Paszke, A., et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information Processing Systems.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento e Ingeniería de Características\n",
    "\n",
    "### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>317</td>\n",
       "      <td>hi im isaac im going writing face mars natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>210</td>\n",
       "      <td>perspective think face natural landform dont t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>105</td>\n",
       "      <td>think face natural landform life mars descover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>362</td>\n",
       "      <td>life mars would know reason think natural land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>101</td>\n",
       "      <td>people thought face formed alieans thought lif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  text_length  \\\n",
       "0                Adequate          317   \n",
       "1                Adequate          210   \n",
       "2                Adequate          105   \n",
       "3                Adequate          362   \n",
       "4                Adequate          101   \n",
       "\n",
       "                                                text  \n",
       "0  hi im isaac im going writing face mars natural...  \n",
       "1  perspective think face natural landform dont t...  \n",
       "2  think face natural landform life mars descover...  \n",
       "3  life mars would know reason think natural land...  \n",
       "4  people thought face formed alieans thought lif...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos preprocesados\n",
    "train_df = pd.read_csv('data/train_preprocessed.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Visualizar las primeras filas del conjunto de entrenamiento\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de Datos\n",
    "\n",
    "Aunque ya se ha realizado un limpieza previa, esta es solo una medida de seguridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "train_df.isnull().sum()\n",
    "\n",
    "# Rellenar o eliminar valores nulos si es necesario\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de Características (Feature Engineering)\n",
    "\n",
    "1. **Tokenización y Normalización del Texto**:\n",
    "   - Convertir texto a minúsculas.\n",
    "   - Eliminar puntuación y caracteres especiales.\n",
    "   - Tokenizar palabras.\n",
    "\n",
    "   > Esto fue realizado en la etapa previa (Normalización)\n",
    "\n",
    "2. **Vectorización**:\n",
    "   - Utilizar TF-IDF para transformar el texto en vectores numéricos.\n",
    "\n",
    "3. **Características Adicionales**:\n",
    "   - Longitud del texto (`text_length`).\n",
    "   - Número de palabras clave específicas relacionadas con elementos argumentativos.\n",
    "\n",
    "### Codificación de Etiquetas (Label Encoding)\n",
    "\n",
    "Antes de dividir los datos, es esencial codificar las etiquetas categóricas en valores numéricos para que los modelos de Scikit-Learn puedan procesarlas correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adequate' 'Effective' 'Ineffective']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inicializar el codificador\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Codificar las etiquetas de clase\n",
    "y = le.fit_transform(train_df['discourse_effectiveness'])\n",
    "\n",
    "# Opcional: Verificar la codificación\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Vectorización del texto\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_text = tfidf.fit_transform(train_df['text'])\n",
    "\n",
    "# Escalar la característica de longitud del texto\n",
    "scaler = StandardScaler()\n",
    "X_length = scaler.fit_transform(train_df[['text_length']])\n",
    "\n",
    "# Concatenar características\n",
    "import scipy.sparse as sp\n",
    "X = sp.hstack([X_text, X_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justificación de las Técnicas Aplicadas\n",
    "\n",
    "- **TF-IDF**: Captura la importancia relativa de las palabras en el contexto del corpus, lo que es esencial para entender la efectividad argumentativa.\n",
    "- **Escalado de Características**: Normaliza la longitud del texto para que no domine otras características durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación de Modelos\n",
    "\n",
    "### División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Inicializar modelos tradicionales\n",
    "log_reg = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)\n",
    "rand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Preparar datos para la Red Neuronal con Keras\n",
    "# No es necesario re-codificar 'y_train' ya está codificada\n",
    "\n",
    "# Definir el modelo de Red Neuronal con Keras\n",
    "def create_nn_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "nn_model_keras = create_nn_model(X_train.shape[1], num_classes)\n",
    "nn_model_keras.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Preparar datos para la Red Neuronal con PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.toarray().astype('float32')\n",
    "        self.y = y.astype('int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset_pytorch = TextDataset(X_train, y_train)\n",
    "val_dataset_pytorch = TextDataset(X_val, y_val)\n",
    "\n",
    "train_loader_pytorch = DataLoader(train_dataset_pytorch, batch_size=128, shuffle=True)\n",
    "val_loader_pytorch = DataLoader(val_dataset_pytorch, batch_size=128, shuffle=False)\n",
    "\n",
    "# Definir el modelo de Red Neuronal con PyTorch\n",
    "class NeuralNetPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "nn_model_pytorch = NeuralNetPyTorch(X_train.shape[1], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y Validación Cruzada\n",
    "\n",
    "#### Modelos Tradicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.6440\n",
      "Random Forest CV Accuracy: 0.6384\n",
      "XGBoost CV Accuracy: 0.6420\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Validación cruzada para Logistic Regression\n",
    "log_reg_cv = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Logistic Regression CV Accuracy: {log_reg_cv.mean():.4f}')\n",
    "\n",
    "# Validación cruzada para Random Forest\n",
    "rand_forest_cv = cross_val_score(rand_forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Random Forest CV Accuracy: {rand_forest_cv.mean():.4f}')\n",
    "\n",
    "# Validación cruzada para XGBoost\n",
    "xgb_cv = cross_val_score(xgb, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'XGBoost CV Accuracy: {xgb_cv.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal con Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.9180 - loss: 0.2380 - val_accuracy: 0.6136 - val_loss: 1.2920\n",
      "Epoch 2/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.9408 - loss: 0.1727 - val_accuracy: 0.6046 - val_loss: 1.4237\n",
      "Epoch 3/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.9568 - loss: 0.1295 - val_accuracy: 0.6175 - val_loss: 1.5757\n",
      "Epoch 4/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9634 - loss: 0.1062 - val_accuracy: 0.5958 - val_loss: 1.6708\n",
      "Epoch 5/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9718 - loss: 0.0840 - val_accuracy: 0.6088 - val_loss: 1.7681\n",
      "Epoch 6/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.9741 - loss: 0.0783 - val_accuracy: 0.6026 - val_loss: 1.8860\n"
     ]
    }
   ],
   "source": [
    "# Definir Early Stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = nn_model_keras.fit(\n",
    "    X_train.toarray(), y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val.toarray(), y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Red Neuronal con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.8440 - Val Loss: 0.7883 - Val Acc: 0.6530\n",
      "Epoch 2/50 - Train Loss: 0.7266 - Val Loss: 0.7949 - Val Acc: 0.6468\n",
      "Epoch 3/50 - Train Loss: 0.6672 - Val Loss: 0.8153 - Val Acc: 0.6355\n",
      "Epoch 4/50 - Train Loss: 0.6017 - Val Loss: 0.8580 - Val Acc: 0.6269\n",
      "Epoch 5/50 - Train Loss: 0.5199 - Val Loss: 0.9311 - Val Acc: 0.6280\n",
      "Epoch 6/50 - Train Loss: 0.4238 - Val Loss: 1.0382 - Val Acc: 0.6130\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir el dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nn_model_pytorch.to(device)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nn_model_pytorch.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "num_epochs = 50\n",
    "early_stop_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    nn_model_pytorch.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader_pytorch:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn_model_pytorch(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader_pytorch.dataset)\n",
    "\n",
    "    # Validación\n",
    "    nn_model_pytorch.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_pytorch:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = nn_model_pytorch(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader_pytorch.dataset)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        # Guardar el mejor modelo\n",
    "        torch.save(nn_model_pytorch.state_dict(), 'best_model_pytorch.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "nn_model_pytorch.load_state_dict(torch.load('best_model_pytorch.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de Hiperparámetros\n",
    "\n",
    "#### Modelos Tradicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Params: {'C': 1, 'solver': 'lbfgs'}\n",
      "Best Random Forest Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best XGBoost Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parámetros para Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(log_reg, param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "print(f'Best Logistic Regression Params: {grid_lr.best_params_}')\n",
    "\n",
    "# Parámetros para Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rand_forest, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(f'Best Random Forest Params: {grid_rf.best_params_}')\n",
    "\n",
    "# Parámetros para XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "print(f'Best XGBoost Params: {grid_xgb.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación y Comparación de Modelos\n",
    "\n",
    "### Evaluación del Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Logistic Regression Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Adequate       0.65      0.85      0.74      4211\n",
      "   Effective       0.66      0.50      0.57      1873\n",
      " Ineffective       0.51      0.19      0.28      1268\n",
      "\n",
      "    accuracy                           0.64      7352\n",
      "   macro avg       0.61      0.51      0.53      7352\n",
      "weighted avg       0.63      0.64      0.61      7352\n",
      "\n",
      "Random Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Adequate       0.64      0.90      0.75      4211\n",
      "   Effective       0.68      0.43      0.52      1873\n",
      " Ineffective       0.59      0.10      0.17      1268\n",
      "\n",
      "    accuracy                           0.64      7352\n",
      "   macro avg       0.64      0.48      0.48      7352\n",
      "weighted avg       0.64      0.64      0.59      7352\n",
      "\n",
      "XGBoost Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Adequate       0.64      0.92      0.75      4211\n",
      "   Effective       0.70      0.41      0.52      1873\n",
      " Ineffective       0.63      0.09      0.16      1268\n",
      "\n",
      "    accuracy                           0.65      7352\n",
      "   macro avg       0.66      0.47      0.48      7352\n",
      "weighted avg       0.65      0.65      0.59      7352\n",
      "\n",
      "Neural Network (Keras) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Adequate       0.66      0.74      0.70      4211\n",
      "   Effective       0.60      0.55      0.57      1873\n",
      " Ineffective       0.40      0.28      0.33      1268\n",
      "\n",
      "    accuracy                           0.61      7352\n",
      "   macro avg       0.55      0.52      0.53      7352\n",
      "weighted avg       0.60      0.61      0.60      7352\n",
      "\n",
      "Neural Network (PyTorch) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Adequate       0.66      0.87      0.75      4211\n",
      "   Effective       0.65      0.54      0.59      1873\n",
      " Ineffective       0.59      0.12      0.20      1268\n",
      "\n",
      "    accuracy                           0.65      7352\n",
      "   macro avg       0.63      0.51      0.51      7352\n",
      "weighted avg       0.64      0.65      0.61      7352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lr = grid_lr.predict(X_val)\n",
    "y_pred_rf = grid_rf.predict(X_val)\n",
    "y_pred_xgb = grid_xgb.predict(X_val)\n",
    "\n",
    "# Predicciones de la Red Neuronal con Keras\n",
    "y_pred_nn_prob_keras = nn_model_keras.predict(X_val.toarray())\n",
    "y_pred_nn_keras = y_pred_nn_prob_keras.argmax(axis=1)  # Mantener etiquetas numéricas\n",
    "\n",
    "# Predicciones de la Red Neuronal con PyTorch\n",
    "nn_model_pytorch.eval()\n",
    "y_pred_nn_pytorch = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in val_loader_pytorch:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = nn_model_pytorch(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred_nn_pytorch.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Ahora, y_pred_nn_pytorch ya está en formato numérico\n",
    "\n",
    "# Reportes de clasificación\n",
    "print(\"Logistic Regression Classification Report\")\n",
    "print(classification_report(y_val, y_pred_lr, target_names=le.classes_))\n",
    "\n",
    "print(\"Random Forest Classification Report\")\n",
    "print(classification_report(y_val, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "print(\"XGBoost Classification Report\")\n",
    "print(classification_report(y_val, y_pred_xgb, target_names=le.classes_))\n",
    "\n",
    "print(\"Neural Network (Keras) Classification Report\")\n",
    "print(classification_report(y_val, y_pred_nn_keras, target_names=le.classes_))\n",
    "\n",
    "print(\"Neural Network (PyTorch) Classification Report\")\n",
    "print(classification_report(y_val, y_pred_nn_pytorch, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualización y Comunicación de Resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
